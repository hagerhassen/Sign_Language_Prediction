{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "from skvideo.io import ffprobe\n",
    "from skvideo.io import vread,vwrite,FFmpegWriter,FFmpegReader\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import joblib"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Fixing Video frames to fit in our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "#function to automate the fixing process\n",
    "def fixVideo(frames,video_name,startFrames=0,endFrames=0,middleFrames=0):\n",
    "    folder_name=video_name.split('\\\\')[0]\n",
    "    file_name=video_name.split('\\\\')[1].split('.')[0]+\"_out.mp4\"\n",
    "    reader=FFmpegReader(video_name)\n",
    "    writer=FFmpegWriter(os.path.join(folder_name,file_name))\n",
    "    counter=0\n",
    "    reachMiddle=False\n",
    "    for frame in reader.nextFrame():\n",
    "        if startFrames!=0:\n",
    "            for i in range(2):\n",
    "                writer.writeFrame(frame)\n",
    "            startFrames-=1\n",
    "        elif middleFrames!=0 and reachMiddle:\n",
    "            for i in range(2):\n",
    "                writer.writeFrame(frame)\n",
    "            middleFrames-=1\n",
    "        elif endFrames!=0 and frames-counter==endFrames:\n",
    "            for i in range(2):\n",
    "                writer.writeFrame(frame)\n",
    "            endFrames-=1\n",
    "        else:\n",
    "            writer.writeFrame(frame)\n",
    "        counter+=1\n",
    "        if isEven(frames):\n",
    "            if frames/counter==2:\n",
    "                reachMiddle=True\n",
    "        if not isEven(frames):\n",
    "            if frames/(counter-0.5)==2:\n",
    "                reachMiddle=True\n",
    "    writer.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#read all video metadata to get frame number of every video\n",
    "data={\"Name\":[],\"Frames\":[]}\n",
    "for dir in os.listdir():\n",
    "    for file in os.listdir(os.path.join(os.curdir,dir)):\n",
    "        if file.endswith(\".mp4\"):\n",
    "            metadata=ffprobe(os.path.join(os.curdir,dir,file))\n",
    "            data[\"Name\"].append(os.path.join(dir,file))\n",
    "            data[\"Frames\"].append(metadata['video']['@nb_frames'])\n",
    "df=pd.DataFrame(data)\n",
    "df[\"Frames\"]=df[\"Frames\"].astype(np.int32)\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "after knowing each video frames number we categorized them and started to handle each category"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#for big videos we used ffmpeg to remove the duplicated frames\n",
    "for video in df[df[\"Frames\"] > 30][\"Name\"]:\n",
    "    folder_name=video.split('\\\\')[0]\n",
    "    file_name=video.split('\\\\')[1].split('.')[0]+\"_out.mp4\"\n",
    "    ret=subprocess.run([\"ffmpeg\",\"-i\",f\"{video}\",\"-vf\",\"mpdecimate,setpts=N/FRAME_RATE/TB\",f\"{os.path.join(folder_name,file_name)}\"])\n",
    "    if ret.returncode==0:\n",
    "        os.remove(video)\n",
    "    else:\n",
    "        print(f\"Error with {video}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# then we choose every category and starting fixing them by our self\n",
    "variable=26 # category with 26 frames\n",
    "for video in df[df[\"Frames\"] == variable][\"Name\"]:\n",
    "    # added total of 4 frames\n",
    "    fixVideo(variable,video,endFrames=1,startFrames=1,middleFrames=2)\n",
    "    #remove the old video as the function will produce new one\n",
    "    os.remove(video)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# then we started increasing video numbers by applying video augmentations\n",
    "import imgaug.augmenters as iaa\n",
    "augs=[iaa.Rotate(5),iaa.Rotate(10),iaa.Rotate(15),\n",
    "      iaa.Rotate(-5),iaa.Rotate(-10),iaa.Rotate(-15),\n",
    "      iaa.ShearX(5),iaa.ShearX(10),iaa.ShearX(-5),\n",
    "      iaa.ShearX(-10),iaa.ScaleY(1.1),iaa.ScaleY(0.9),\n",
    "      iaa.TranslateX(px=5),iaa.TranslateY(px=5),\n",
    "      iaa.Sequential([iaa.TranslateY(px=5),iaa.TranslateX(px=5)])]\n",
    "for video in df[\"Video\"]:\n",
    "    video_file=vread(video)\n",
    "    output=aug.augment_images(video_file)\n",
    "    vwrite(f'{video.split(\".\")[0]}_filp.mp4',output)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# then we began our feature extraction process using mediapipe\n",
    "import mediapipe as mp\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_holistic = mp.solutions.holistic"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# started by writing our feature extraction function which extracts specific points from the pose estimator and the hand estimator\n",
    "def extract_keypoints(results):\n",
    "    la = np.array([[res.x, res.y, res.z] if res.visibility > 0.2 else [0,0,0] for res in np.array(results.pose_landmarks.landmark)[[13,15]]]) if results.pose_landmarks else np.zeros((2,3))\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]) if results.left_hand_landmarks else np.zeros((21,3))\n",
    "    ra = np.array([[res.x, res.y, res.z] if res.visibility > 0.2 else [0,0,0] for res in np.array(results.pose_landmarks.landmark)[[14,16]]]) if results.pose_landmarks else np.zeros((2,3))\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]) if results.right_hand_landmarks else np.zeros((21,3))\n",
    "    return np.concatenate([la ,lh ,ra , rh])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# then we prepared dataset with our videos\n",
    "data={\"Name\":[],\"Label\":[]}\n",
    "for dir in os.listdir():\n",
    "    if os.path.isdir(os.path.join(os.curdir,dir)):\n",
    "        for video in os.listdir(dir):\n",
    "            if video.endswith(\".mp4\"):\n",
    "                data[\"Name\"].append(os.path.join(dir,video))\n",
    "                data[\"Label\"].append(dir)\n",
    "df=pd.DataFrame(data)\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#started our feature extraction process by reading video and extract its features and then saves them on npy files in the disk\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.001,min_tracking_confidence=0.001) as holistic:\n",
    "    for video in tqdm(df[\"Name\"]):\n",
    "        if not os.path.isfile(video.split(\".\")[0]+\".npy\"):\n",
    "            reader=FFmpegReader(video)\n",
    "            results_arr=[]\n",
    "            for frame in reader.nextFrame():\n",
    "                results=holistic.process(frame)\n",
    "                results_arr.append(extract_keypoints(results))\n",
    "            temp_arr=np.array(results_arr)\n",
    "            np.save(video.split(\".\")[0],temp_arr)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# then we concatenated\n",
    "X=np.load(df[\"Name\"][0]).reshape((1,30,46,3))\n",
    "labels=[df[\"Label\"][0]]\n",
    "for data,label in tqdm(list(zip(df[\"Name\"][1:],df[\"Label\"][1:]))):\n",
    "    temp=np.load(data)\n",
    "    if np.all(temp==0):\n",
    "        continue\n",
    "    temp=temp.reshape((1,30,46,3))\n",
    "    X=np.concatenate([X,temp],axis=0)\n",
    "    labels.append(label)\n",
    "\n",
    "y=np.array(labels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# then encoded the labels\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder=LabelEncoder()\n",
    "y=encoder.fit_transform(y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# then saved our features and targets for future use with the encoder\n",
    "np.save(\"Features\",X)\n",
    "np.save(\"Target\",y)\n",
    "joblib.dump(encoder,\"encoder.pkl\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# we split the data to train,test and valid datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.1,random_state=42,stratify=y)\n",
    "X_train,X_valid,y_train,y_valid=train_test_split(X_train,y_train,test_size=0.2,random_state=42,stratify=y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential,load_model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Input, Dropout , LeakyReLU,Conv1D,MaxPool1D,GlobalMaxPool1D,TimeDistributed,Reshape,BatchNormalization\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping,TensorBoard"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# then started writing our two model\n",
    "model=Sequential(name=\"CNNLSTM\")\n",
    "model.add(TimeDistributed(Conv1D(64,kernel_size=3,padding=\"same\",activation=\"relu\"),input_shape=X_train.shape[1:]))\n",
    "model.add(TimeDistributed(MaxPool1D()))\n",
    "model.add(TimeDistributed(Conv1D(96,kernel_size=3,padding=\"same\",activation=\"relu\")))\n",
    "model.add(TimeDistributed(MaxPool1D()))\n",
    "model.add(TimeDistributed(Conv1D(128,kernel_size=3,padding=\"same\",activation=\"relu\")))\n",
    "model.add(TimeDistributed(GlobalMaxPool1D()))\n",
    "model.add(LSTM(90,dropout=0.4,return_sequences=True))\n",
    "model.add(LSTM(45,dropout=0.4))\n",
    "model.add(Dense(100,activation=\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(50,activation=\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(np.unique(y).shape[0],activation=\"softmax\"))\n",
    "model.compile(optimizer=\"nadam\",loss=\"sparse_categorical_crossentropy\",metrics=['accuracy'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model2=Sequential(name=\"LstmModel\")\n",
    "model2.add(LSTM(256,return_sequences=True,input_shape=X_train_lstm.shape[1:]))\n",
    "model2.add(LSTM(128,dropout=0.3))\n",
    "model2.add(Dense(100,activation=\"relu\"))\n",
    "model2.add(Dropout(0.2))\n",
    "model2.add(Dense(128,activation=\"relu\"))\n",
    "model2.add(Dropout(0.2))\n",
    "model2.add(Dense(np.unique(y).shape[0],activation=\"softmax\"))\n",
    "model2.compile(optimizer=\"nadam\",loss=\"sparse_categorical_crossentropy\",metrics=['accuracy'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# for model 1 we didn't need any data preparation because it's responsible to extract features and then map it to the lstm\n",
    "# but for model 2 we make two data preparation one with pca and the second with flatten the features\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "IPCA=IncrementalPCA(n_components=1)\n",
    "for video in tqdm(X_train):\n",
    "    for frame in video:\n",
    "        IPCA.partial_fit(frame)\n",
    "\n",
    "def return_pca(data):\n",
    "    pca=[]\n",
    "    for video in tqdm(data):\n",
    "        frames=[]\n",
    "        for frame in video:\n",
    "            frames.append(IPCA.transform(frame))\n",
    "        pca.append(np.array(frames))\n",
    "    return np.array(pca).reshape((-1,30,46))\n",
    "\n",
    "X_train_pca=return_pca(X_train)\n",
    "X_test_pca=return_pca(X_test)\n",
    "X_valid_pca=return_pca(X_valid)\n",
    "\n",
    "X_train_lstm=X_train.reshape((-1,30,3*46))\n",
    "X_test_lstm=X_test.reshape((-1,30,3*46))\n",
    "X_valid_lstm=X_valid.reshape((-1,30,3*46))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# after training we found out that the cnn model is the most accurate one"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\GP\\Final\\Model\n"
     ]
    }
   ],
   "source": [
    "cd D:\\GP\\Final\\Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X=np.load(\"Features.npy\")\n",
    "y=np.load(\"Target.npy\")\n",
    "encoder=joblib.load(\"encoder.pkl\")\n",
    "model=load_model(\"Final_model.h5\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131/131 [==============================] - 6s 6ms/step - loss: 0.0028 - accuracy: 0.9990\n"
     ]
    },
    {
     "data": {
      "text/plain": "[0.002786200726404786, 0.9990403056144714]"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_train,y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 [==============================] - 0s 12ms/step - loss: 0.0014 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": "[0.0014376712497323751, 1.0]"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_valid,y_valid)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 0s 22ms/step - loss: 0.0265 - accuracy: 0.9965\n"
     ]
    },
    {
     "data": {
      "text/plain": "[0.026527803391218185, 0.9965457916259766]"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test,y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}